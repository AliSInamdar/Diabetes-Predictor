# -*- coding: utf-8 -*-
"""Ali_Inamdar_Project1_IntroToDS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DC16JTEkFkuvbG7YXOIlS8EzKR55qYVi

## Ali Inamdar

## Intro To DS - CS675
##  -Project 1
"""

!pip install scikit-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

class Diabetes():


    def __init__(self):
        pass


    def get_info(self, df):
      print("Names of all the columns: \n")
      print(df.columns)
      print("\nData types of all the columns: \n")
      print(df.dtypes)
      print("\nGeneral information about the dataframe: \n")
      print(df.info())

    def clean_data(self, df1, df2):
        print("Checking for Null and NaN values in each column:\n")
        print(df1.isnull().sum())
        # 2. Check if there are any NaN values in the entire dataset
        print("\nTotal NaN values in the entire dataset: ", df1.isna().sum().sum())

        # 3. Display rows that have any NaN values (if any)
        print("\nRows with NaN values:")
        nan_rows = df1[df1.isna().any(axis=1)]
        print(nan_rows)

    def outliers(self, df):
        numeric_df = df.select_dtypes(include=np.number)

        Q1 = numeric_df.quantile(0.25)
        Q3 = numeric_df.quantile(0.75)
        IQR = Q3 - Q1

        # Step 2: Calculate the lower and upper bounds for outlier detection
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Step 3: Find outliers in the dataset
        outliers = ((numeric_df < lower_bound) | (numeric_df > upper_bound))

        # Display the count of outliers in each column
        print("Number of outliers in each column:\n", outliers.sum())

        # Optional: Display rows with outliers
        print("\nRows containing outliers:\n")
        print(df[outliers.any(axis=1)])

    def list_of_datatypes(self, df):
        print("Data Types of Each Column:\n")
        print(df.dtypes)
        # Identify numeric columns
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
        # Identify categorical columns
        categorical_columns = df.select_dtypes(include=['object']).columns
        # Step 3: Print the categorized columns
        print("\nNumeric Columns:\n", numeric_columns)
        print("\nCategorical Columns:\n", categorical_columns)

        # Step 4: Handle mixed data types if necessary (optional step)
        # If there are columns with mixed data types, they can be listed and handled separately
        mixed_columns = [col for col in df.columns if len(df[col].apply(type).unique()) > 1]
        print("\nColumns with Mixed Data Types (if any):\n", mixed_columns)

    def check_correlation(self, df):
        print("Correlation Matrix:\n")
        correlation_matrix = data.corr()
        print(correlation_matrix)

        # Visualizing the Correlation Matrix using a Heatmap
        plt.figure(figsize=(12, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
        plt.title("Correlation Heatmap of Features")
        plt.show()

        # Step 2: Pairplot to Visualize Relationships between Features
        # This helps in visualizing dependencies between pairs of features
        sns.pairplot(data, hue='Outcome')
        plt.title("Pair Plot of Features Grouped by Outcome")
        plt.show()

        # Step 3: Feature Importance Using Random Forest
        # Separating Features and Target Variable
        X = data.drop(columns=['Outcome'])  # Features
        y = data['Outcome']                # Target Variable

        # Standardizing the Features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Creating and Fitting a Random Forest Model
        rf_model = RandomForestClassifier(random_state=42)
        rf_model.fit(X_scaled, y)

        # Extracting Feature Importances
        importances = rf_model.feature_importances_
        feature_names = X.columns

        # Creating a DataFrame for Feature Importance
        feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

        # Display Feature Importance
        print("\nFeature Importance (from Random Forest):\n", feature_importance_df)

        # Step 4: Visualizing Feature Importance
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
        plt.title('Feature Importance')
        plt.show()

    def feature_selection(self, df):
        X = df.drop(columns=['Y'])
        y = df['Y']

        # Step 2: Standardize the Features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Step 3: Train a Random Forest Classifier
        rf_model = RandomForestClassifier(random_state=42)
        rf_model.fit(X_scaled, y)

        # Step 4: Calculate Feature Importances
        importances = rf_model.feature_importances_
        feature_names = X.columns

        # Step 5: Create a DataFrame for Feature Importance
        feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

        # Display Feature Importance
        print("\nTop Features Affecting the Outcome:\n")
        print(feature_importance_df)

        # Step 6: Plot Feature Importance for Visualization
        import matplotlib.pyplot as plt
        import seaborn as sns

        # Visualizing Feature Importance
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
        plt.title('Feature Importance for Diabetes Outcome')
        plt.xlabel('Importance Score')
        plt.ylabel('Feature')
        plt.show()

    def pair_plot(self, df):
        chosen_columns = ['AGE','SEX', 'BMI','BP','S1','S2','S3','S4','S5','S6','Y']
        sns.pairplot(df[chosen_columns])
        plt.show()

    def contingency_table(self,df):
        #create a contingency table to see relationships between the behaviors and age of the squirrel
        contingency_table = pd.crosstab(df['Y'], columns=[df['AGE'], df['SEX'],df['BMI'],df['BP']], margins=True)
        pd.set_option('display.max_columns', None)
        #contingency_table_df = contingency_table.reset_index()
        print(contingency_table)

"""Displaying the NULL and NAN values."""

diabetes_df = pd.read_csv('/content/Maindata.csv')
diabetes_df_copy = diabetes_df.copy()
diabetes = Diabetes()
diabetes.get_info(diabetes_df)
#perform methods on the instance
diabetes.clean_data(diabetes_df, diabetes_df_copy)
diabetes.get_info(diabetes_df)

"""Displaying the Dataset."""

diabetes_df

"""Displaying the Outliers."""

diabetes.outliers(diabetes_df)

"""Displaying the List of Datatypes present in the CSV."""

diabetes.list_of_datatypes(diabetes_df)

"""## Performing EDA

Showing the correlation amongst various features and data.
"""

diabetes_df.corr()

diabetes_df.describe()

diabetes_df.info()

diabetes.feature_selection(diabetes_df)

diabetes.pair_plot(diabetes_df)

diabetes.contingency_table(diabetes_df)

"""4- State limitations/issues (if any) with the given dataset.


- Had an issue changing the csv into a standard form as it was having only one header and wasn't able to perform proper EDA, but apart from that there were no issues in the dataset given.

- All values present in the dataset were already numeric , so had no issues in performing any tasks.
"""